# **Joseph Salazar**  
ğŸ“§ Jaspenny7@gmail.com | [LinkedIn](https://www.linkedin.com/in/jsalazar7) | [GitHub](https://github.com/japenny)

---

## **Education**

**University of Texas at El Paso**  
_Bachelor of Science in Computer Science_  
**Major GPA:** 3.86  
ğŸ“… Dec 2025

---

## **Work Experience**

**Google**  
_Software Engineer Intern_  
ğŸ“ Mountain View, California  
ğŸ“… Aug 2024 â€“ Nov 2024  

- Pioneered a comprehensive experimental LLM-based pipeline to effectively detect hallucinations in bilingual machine translation outputs.
- Engineered a Python-based preprocessing pipeline with NumPy and parallelized core pipeline modules, accelerating model training throughput.
- Improved pipeline consistency and reliability by deploying 4+ research-supported prompt engineering methodologies.  
- Boosted model evaluation robustness by integrating a novel quantitative metric, which drove a 15%+ uplift in model accuracy.  
- Directed a 6-person human rater study, producing a gold-standard NLP dataset of 400+ data points for linguistic translation evaluation.  
- Increased inter-rater reliability to 0.85+ across 1200+ annotations by designing a rigorous study protocol, ensuring data integrity for future ML research.  

**Microsoft**  
_Software Engineer Intern_  
ğŸ“ Atlanta, Georgia  
ğŸ“… May 2024 â€“ Aug 2024  

- Elevated internal stakeholder efficiency by 30% through exploratory research and validation of an AI-driven NLP report automation tool.  
- Cut AI system errors by 50% via targeted prompt engineering, improving output reliability for critical workflows.  
- Accelerated development timeline by evaluating 6 Azure AI/ML services and delivering 5+ key tooling recommendations.  

---

## **Professional Development**

**Google Tech Exchange Program**  
ğŸ“… Jan 2023 â€“ May 2023  

- Advanced technical acumen in data structures, software design, and documentation with the guidance of Google engineers.  
- Led the team workflow for a 4-person team developing an online platform and established clear communication standards, leading to improved collaboration.  

**Stanford Online: Intro to Deep Learning**  
ğŸ“… Feb 2022 â€“ June 2022  

- Engineered lower-level TensorFlow models, encompassing tensors, generators, custom layers, and training loops.  
- Leveraged TensorFlowâ€™s Probability library for quantifying noise, uncertainty, and generative AI modeling.  
- Improved neural network generalization by applying techniques such as regularization, hyperparameter tuning, and attention mechanisms.  

---

## **Projects**

**Multilingual Toxic Comments Model**  
_Tools: Git, TensorFlow, NumPy, Pandas_  
- Development of 4 experimental machine learning models for detecting toxicity across 30,000 comments.  
- Implemented models utilizing transformer architecture, transfer learning, and pre-trained multilingual embedding, achieving 82% classification accuracy.  

**Abstractive Text Summarization Model**  
_Tools: Keras, Natural Language Processing (NLP)_  
- Assembled a text summarization model with Keras functional API that processed 100,000+ documents and retained 50%+ of vital information in summaries.  

---

## **Technical Skills**

**Languages:** Python (3 yrs), Java (2 yrs), C (2 yrs), RISCV Assembly (1 yr), C++ (<1 yr)  
**ML/AI Frameworks:** TensorFlow, Keras, Scikit-Learn, Hugging Face, NumPy, Pandas  
**Developer Tools:** Git, Google Colab, LaTeX, Google Cloud Platform, CI/CD, PyCharm, IntelliJ  
**Certificates:** Codepath/Meta Cybersecurity, Harvard Online Intro to Applied Probability  
**Interests:** Chess, Track, Piano, Puzzle Games
